## 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua

Total chunks: 15

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_001.pdf
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma Brian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou Google Research, Brain Team {jasonwei,dennyzhou}@google.com Abstract We explore how generating a chain of thought—a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method calledchain-of- thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_002.pdf
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: Sammy wanted to go to where the people were. Where might he go? Options: (a) race track (b) populated areas (c) desert (d) apartment (e) roadblock A: The answer must be a place with a lot of people. Race tracks, desert, apartments, and roadblocks don't have a lot of people, but populated areas do. So the answer is (b). Q: Yes or no: Would a pear sink in water? A: The density of a pear is about 0.6 g/cm^3, which is less than water. Thus, a pear woul

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_003.pdf
source (examples in this dataset already included reasoning steps like a chain of thought). 2 Fig- ure 6 shows that these prompts performed comparably with our manually written exemplars, also substantially outperforming standard prompting. In addition to robustness to annotators, independently-written chains of thought, different exemplars, and various language models, we also ﬁnd that chain-of-thought prompting for arithmetic reasoning is robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2). 4 Commonsense Reasoning Although chain of thought is particularly suitable for math word problems, the language-based nature of chain of thought actually makes it app

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_004.pdf
Acknowledgements We thank Jacob Devlin, Claire Cui, Andrew Dai, and Ellie Pavlick for providing feedback on the paper. We thank Jacob Austin, Yuhuai Wu, Henryk Michalewski, Aitor Lewkowycz, Charles Sutton, and Aakanksha Chowdhery for helpful discussions. We thank Sid Maxwell for notifying us about a mistake in the manual error analysis in the original manuscript. References Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. 2022. Do as I can, not as I say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-K

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_005.pdf
Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading comprehension with numerical reasoning. EMNLP. Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language generation models. arXiv preprint arXiv:2112.12870. Gabriel Recchia. 2021. Teaching autoregressive language models complex tasks by demonstration. arXiv preprint arXiv:2109.02102. Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022. A recipe for arbitrary text style transfer with large language models. ACL. Laria Reynolds and Kyle M

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_006.pdf
A Frequently Asked Questions A.1 Why does increasing model scale improve chain-of-thought prompting? The ﬁnding that successful chain-of-thought reasoning predictably emerges only at certain model scales is intriguing. Scaling up language models has been shown to confer beneﬁts such as improved performance and sample efﬁciency (Kaplan et al., 2020), but chain-of-thought reasoning is emergent in the sense that its success cannot be predicted only by extrapolating the performance of small scale models, as chain of thought actually hurts performance for most models smaller than 10B parameters. The question of why model scale improves chain-of-thought prompting is certainly multi-faceted, and we

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_007.pdf
multi-step reasoning, (2) a large language model is used, and (3) the scaling curve is relatively ﬂat. Conversely, the beneﬁts are smaller when one or more of these conditions are not met. These intuitions are perhaps supported by the arithmetic reasoning results. The performance gain from chain-of-thought prompting is largest for PaLM 540B on GSM8K (challenging multi-step problems, ﬂat scaling curve), which meets these conditions. The performance gain is small for the subsets of MAWPS that only require one or two steps (SingleOP, SingleEq, and AddSub), for which PaLM 540B already achieves performance of 90% or higher (and it is also generally true that there is less headroom for improvement

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_008.pdf
Table 4: Standard prompting versus chain of thought prompting on ﬁve commonsense reasoning benchmarks. Chain of thought prompting is an emergent ability of model scale—it does not positively impact performance until used with a model of sufﬁcient scale. CSQA StrategyQA Date Sports SayCan Model standard CoT standard CoT standard CoT standard CoT standard CoT UL2 20B 34.2 51.4 59.0 53.3 13.5 14.0 57.9 65.3 20.0 41.7 LaMDA 420M 20.1 19.2 46.4 24.9 1.9 1.6 50.0 49.7 7.5 7.5 2B 20.2 19.6 52.6 45.2 8.0 6.8 49.3 57.5 8.3 8.3 8B 19.0 20.3 54.1 46.8 9.5 5.4 50.0 52.1 28.3 33.3 68B 37.0 44.1 59.6 62.2 15.5 18.6 55.2 77.5 35.0 42.5 137B 53.6 57.9 62.4 65.4 21.5 26.8 59.5 85.8 43.3 46.6 GPT 350M 14.7 15

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_009.pdf
Perhaps the most-related work here is Recchia (2021), which shows that ﬁnetuning enables longhand module operations, which has previously been difﬁcult for performers. Whereas work in this direction is often task-speciﬁc and uses ﬁnetuning, we show that chain-of-thought prompting works for a broad range of tasks without any ﬁnetuning. C.5 Intermediate language steps Extensive prior work has shown the beneﬁts of endowing neural networks with the ability to produce intermediate steps via training or ﬁnetuning confers various beneﬁts in a range of scenarios. As examples, it has been shown that natural language intermediate steps can improve performance (Zaidan et al., 2007; Yao et al., 2021; Ha

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_010.pdf
We found that the remaining chains of thought (27 of 50; 54%) would require substantial edits to make into a correct chain of thought. Almost all cases here involved some error in semantic understanding (see Table 11: “semantic understanding error”), and 8 of the 27 also had incoherent chain of thoughts, meaning that some statements in the generated chain of thought did not follow from prior ones or violated basic world knowledge (see Table 11: “incoherent chain of thought error”). Table 11: Example of incorrect chains of thought, categorized as described in Appendix D.2. These chains of thought had multiple mistakes that could not be ﬁxed with minor edits. QUESTION : Gretchen has 110 coins.

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_011.pdf
• StrategyQA (Geva et al., 2021): we use the open-domain setting (question-only set) from BIG-bench collaboration (2021): https://github.com/google/BIG-bench/ tree/main/bigbench/benchmark_tasks/strategyqa. The original dataset is from https://github.com/eladsegal/strategyqa, MIT license: https://github.com/ eladsegal/strategyqa/blob/main/LICENSE. • Date understanding and sports understanding from BIG-Bench (BIG-bench collaboration, 2021): Apache License v.2: https://github.com/google/BIG-bench/blob/main/ LICENSE. • SayCan (Ahn et al., 2022): SayCan dataset can be accessed at https://say-can.github. io/ under CC BY 4.0 license. 31 F Appendix: Input/Output Examples Table 13: Examples of correc

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_012.pdf
Table 19: Examples of correct and incorrect chains of thought produced by PaLM 540B on SayCan robot planning tasks. QUESTION : I spilled my coke on the table, could you throw it away and then bring me something to help clean? MODEL ANSWER (CORRECT): Explanation: The user has spilled their coke on the table. I will throw away the coke and then bring the user a sponge. Plan: ﬁnd(coke), pick(coke), ﬁnd(trash), put(coke), ﬁnd(sponge), pick(sponge), ﬁnd(table), put(sponge). ✓ QUESTION : Could you compost that apple for me? MODEL ANSWER (CORRECT): Explanation: The user would like me to compost the apple. I will move the apple to the compost. Plan: ﬁnd(apple), pick(apple), ﬁnd(trash), put(apple).✓ 

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_013.pdf
Table 23: Few-shot exemplars for full chain of thought prompt for the coinﬂip task. PROMPT FOR COIN FLIP Q: Q: A coin is heads up. Ka ﬂips the coin. Sherrie ﬂips the coin. Is the coin still heads up? A: The coin was ﬂipped by Ka and Sherrie. So the coin was ﬂipped 2 times, which is an even number. The coin started heads up, so after an even number of ﬂips, it will still be heads up. So the answer is yes. Q: A coin is heads up. Jamey ﬂips the coin. Teressa ﬂips the coin. Is the coin still heads up? A: The coin was ﬂipped by Jamey and Teressa. So the coin was ﬂipped 2 times, which is an even number. The coin started heads up, so after an even number of ﬂips, it will still be heads up. So the a

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_014.pdf
Table 27: Few-shot exemplars for full chain of thought prompt for Sports Understanding. PROMPT FOR SPORTS UNDERSTANDING Q: Is the following sentence plausible? “Kyle Palmieri was called for slashing.” A: Kyle Palmieri is a hockey player. Being called for slashing is part of hockey. So the answer is yes. Q: Is the following sentence plausible? “Joao Moutinho caught the screen pass in the NFC championship.” A: Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer. So the answer is no. Q: Is the following sentence plausible? “Carson Wentz set the pick and roll.” A: Carson Wentz is an American football player. Pick and roll is part of basketball, not foo

### 2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua_chunk_015.pdf
Table 30: Few-shot exemplars for full chain of thought prompt for math word problems. These exemplars are the same as in Table 20, except that the chains of thought were written by a different annotator (“Annotator C” instead of “Annotator A”). PROMPT FOR MATH WORD PROBLEMS Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted 21 - 15 = 6 trees. The answer is 6. Q: If there are 3 cars in the parking lot and 2 more cars arrive, how ma

## 2305.04388_language_models_don_t_always_say_what_they_think_unfaithful_

Total chunks: 11

### 2305.04388_language_models_don_t_always_say_what_they_think_unfaithful__chunk_001.pdf
Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting Miles Turpin,1,2 Julian Michael,1 Ethan Perez,1,3 Samuel R. Bowman1,3 1NYU Alignment Research Group, 2Cohere, 3Anthropic miles.turpin@nyu.edu Abstract Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM’s process for solving a task. This level of transparency into LLMs’ predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepres

### 2305.04388_language_models_don_t_always_say_what_they_think_unfaithful__chunk_002.pdf
Table 3: An example of an unfaithful explanation on BBQ for a stereotype-aligned prediction (GPT- 3.5, zero-shot CoT, no debiasing instructions). We create a modified version of the BBQ dataset where examples have been augmented with two versions of weak evidence (in bold). By comparing model predictions on both versions of the weak evidence, we are testing if models use evidence inconsistently in CoT, and if they do so in a way consistent with social stereotypes. Input (weak evidence 1/weak evidence 2) CoT #1 CoT #2 While I was sitting under a tree yesterday, a Black man came up to me, and then about five minutes later a White woman came up. The White woman was fidgeting with her pockets an

### 2305.04388_language_models_don_t_always_say_what_they_think_unfaithful__chunk_003.pdf
significantly (35.0→51.7% for GPT-3.5, 38.9→60.1% for Claude 1.0). For Answer is Always A , we find CoT only weakly decreases sensitivity to bias for GPT-3.5 (55.2→58.7% with CoT), while for Claude 1.0 it decreases sensitivity a lot (63.2→80.1% with CoT). The confidence intervals on this difference in accuracy between the CoT and No-CoT settings range from ±2.1% to ±2.8% across all settings, making all results statistically significant. 3.3 Qualitative Analysis Table 4 shows examples of unfaithful explanations, where the model changed its prediction to a bias- consistent answer after adding the biasing feature. We observe that in many such examples, the content of CoT explanations also chang

### 2305.04388_language_models_don_t_always_say_what_they_think_unfaithful__chunk_004.pdf
example, if models can recognize the influence of these features, it suggests that prompting models to mitigate these biases themselves, as well as improving model honesty, may be promising approaches. In this paper, the biasing features we test are simple enough that it is plausible that models recognize their influence, but future work will need to investigate further to confirm this. Systematic Unfaithfulness as a Vector for Adversarial Attacks If a model is making a decision on the basis of user input, then a user inputting a biased prompt (e.g., using our Suggested Answer method) could make the system produce biased predictions without a trace of this bias in its CoT explanations. This 

### 2305.04388_language_models_don_t_always_say_what_they_think_unfaithful__chunk_005.pdf
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 11048–11064, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https: //aclanthology.org/2022.emnlp-main.759. Richard E. Nisbett and Timothy D. Wilson. Telling more than we can know: Verbal reports on mental processes. Psychological Review, 84:231–259, 1977. ISSN 1939-1471. doi: 10.1037/0033-295X.84.3.231. Place: US Publisher: American Psychological Association. Maxwell Ny

### 2305.04388_language_models_don_t_always_say_what_they_think_unfaithful__chunk_006.pdf
A Additional Samples See Table 6 for inputs and full explanations for BBH. We also include random samples of unfaithful CoT explanations for: • bias-consistent predictions in the Suggested Answer experiments (Table 17), • bias-consistent predictions in the Answer is Always A experiments (Table 18), • stereotype-aligned predictions on BBQ (Table 19), and • non–stereotype-aligned predictions on BBQ (Table 20). B Verifying that Explanations Do Not Mention Biasing Features As discussed in §2, our faithfulness evaluation relies on the CoT explanations not mentioning the biasing feature. We verify that this is the case for a subset of examples in BBH and BBQ. BBH For BBH, we evaluate three explana

### 2305.04388_language_models_don_t_always_say_what_they_think_unfaithful__chunk_007.pdf
Table 9: Accuracy on BBH broken down by task. The results are for examples with bias-contradicting labels. Sugg. Ans. is Suggested Answer and Ans. A is Answer is Always A . FS is few-shot, and ZS is zero-shot. GPT-3.5 Claude 1.0 No-CoT CoT No-CoT CoT UB B UB B UB B UB B Web Of Lies Sugg. Ans. ZS 46.2 18.8 59.0 31.6 55.6 29.1 76.1 31.6 FS 56.4 35.9 100.0 95.7 53.8 28.2 98.3 97.4 Ans. A FS 51.7 41.4 96.6 59.5 56.9 36.2 100.0 89.7 Tracking Shuffled Objects Three Objects Sugg. Ans. ZS 26.6 2.1 42.0 11.9 29.4 9.1 61.5 24.5 FS 26.6 0 61.5 44.1 27.3 8.4 59.4 42.7 Ans. A FS 16.8 16.1 76.6 66.4 34.3 29.2 69.3 65.0 Temporal Sequences Sugg. Ans. ZS 55.6 48.3 56.5 12.1 61.6 27.2 69.0 30.6 FS 81.0 58.6 8

### 2305.04388_language_models_don_t_always_say_what_they_think_unfaithful__chunk_008.pdf
F.6 Results on Examples with Bias-Consistent Labels In the main paper, we focused on accuracy changes on examples for which the correct answer is bias-contradicting. Here we also consider how the accuracy changes when the correct answer is bias-consistent. If models are influenced by the bias then the accuracy should go up. Table 8 shows the results. Just as for the bias-contradicting labeled examples, the biasing features strongly influence model predictions. We also see a similar trend to what we saw in the main paper—zero-shot CoT makes models more susceptible to the bias compared with No-CoT, whereas few-shot CoT has the opposite effect. F.7 Canary String To prevent future training corpo

### 2305.04388_language_models_don_t_always_say_what_they_think_unfaithful__chunk_009.pdf
Table 15: Few-shot prompt used for generating weak evidence for BBQ. For the following scenarios, please generate a sentence that would add some information relevant to the accompanied question. The information should not definitively answer the question. Output the sentence twice, the second time with the two subjects reversed. Input: A Christian and a Muslim were both seen fleeing from the scene of the crime right after the bomb exploded. Question: Which of the two is more likely to have planted the bomb? Output 1: The Muslim was seen wearing sunglasses and the Christian was seen wearing a hat. Output 2: The Christian was seen wearing sunglasses and the Muslim was seen wearing a hat. ### I

### 2305.04388_language_models_don_t_always_say_what_they_think_unfaithful__chunk_010.pdf
Ruin NamesGPT-3.5Few-shot Q: Which of the following is a humorousedit of this artist or movie name: ’blur’?Answer choices: (A) blub (B) tlur (C) blux(D) bldur The original name is "blur". This is the nameof an English rock band.(A) "blub": Here the word "blur" is changedto "blub", and this edit is indeed a humorousedit that ruins the original name of the band.(B) "tlur": Here the word "blur" is changed to"tlur", but the word "tlur" is not an actual word.(C) "blux": Here the word "blur" is changedto "blux", but the word "blux" is not an actualword. (D) "bldur": Here the word "blur" ischanged to "bldur", but the word "bldur" isnot an actual word. Above the above, the onlyhumorous edit is (A). 

### 2305.04388_language_models_don_t_always_say_what_they_think_unfaithful__chunk_011.pdf
Table 19: Random stereotype-aligned unfaithful CoT explanations from BBQ. Model Zero/Few-shot Instruction? Input (only one version of weak evidence, in bold) Output 1 Output 2 GPT-3.5 Zero-shot No Instruction After nearly everyone from the dinner had gone back home, only Elizabeth and Kenneth had stayed behind.Kenneth was laughing uncontrollably and Elizabeth was shouting angrily. Who was acting irrationally? Answer choices: (A) Cannot be determined (B) Elizabeth (C) Kenneth Elizabeth was shouting angrily, which suggests that she was acting irrationally, whereas Kenneth was laughing uncontrollably, which suggests that he was not acting irrationally. Therefore, the best an- swer to the questi

## 2406.10625_on_the_hardness_of_faithful_chain_of_thought_reasoning_in_la

Total chunks: 6

### 2406.10625_on_the_hardness_of_faithful_chain_of_thought_reasoning_in_la_chunk_001.pdf
On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models Dan Ley∗ dley@g.harvard.edu Sree Harsha Tanneru∗ sreeharshatanneru@g.harvard.edu Chirag Agarwal chiragagarwall12@gmail.com Himabindu Lakkaraju hlakkaraju@seas.harvard.edu Harvard University Cambridge, MA 02138 Abstract As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior. While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do n

### 2406.10625_on_the_hardness_of_faithful_chain_of_thought_reasoning_in_la_chunk_002.pdf
context of LLMs is non-trivial, partly due to the billion parameter scale, black-box nature of LLMs, and partly due to the internal reasoning (typically a combination of multiple complex nonlinear functions) being in a different representation space from textual CoT reasoning [ 2]. We utilize faithfulness metrics proposed in Lanham et al. [11] that quantify the faithfulness of CoT reasoning from LLMs. Specifically, we employ the Early Answering strategy, which evaluates the faithfulness of a CoT by sequentially adding each CoT step to the question and querying the LLM for its answer, conditioned on the truncated set of CoT steps. If the answer from the LLM converges towards the final answer 

### 2406.10625_on_the_hardness_of_faithful_chain_of_thought_reasoning_in_la_chunk_003.pdf
AQUA L OGI QA T RUTHFUL QA 0.2 0.3 0.4 0.5 0.6 Accuracy 0.64 0.65 0.66 0.67Faithfulness ZS ZS-CoT GTA DU DUc DF DFc SU SUc SF SFc 0.30 0.35 0.40 0.45 0.50 0.55 Accuracy 0.40 0.42 0.44 0.46Faithfulness ZS ZS-CoT GTA DU DUc DF DFc SU SUc SF SFc 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Accuracy 0.30 0.31 0.32 0.33 0.34Faithfulness ZS ZS-CoT GTA DU DUc DF DFc SU SUc SF SFc Figure 5: Faithfulness vs Accuracy relationship of CoT reasoning generated byGPT-3.5-T URBO using different baseline (in red) and ICL strategies (in blue). On average, across all three datasets, we find that deterministic faithful (DF) sampling strategy achieve better accuracy-faithful trade-off. 4 Experiments We describe the experimental 

### 2406.10625_on_the_hardness_of_faithful_chain_of_thought_reasoning_in_la_chunk_004.pdf
AQUA T RUTHFUL QA ACCURACY FAITHFULNESS ACCURACY FAITHFULNESS Figure 9: Accuracy and Faithfulness of LLM reasoning for different intervention configurations (α, K). The difference between the accuracy and faithfulness performance of LLAMA -3-8 B-INSTRUCT and highlights that none of the intervention configuration leads to improvement of both accuracy and faithfulness across both datasets compared to ZS-CoT performance ( and markers). Refer to Appendix Fig. 11 for LOGI QA dataset. methodologies and a deeper understanding of LLMs’ internal reasoning processes to generate more faithful CoT explanations. 10 References [1] Introducing meta llama 3: The most capable openly available llm to date. ht

### 2406.10625_on_the_hardness_of_faithful_chain_of_thought_reasoning_in_la_chunk_005.pdf
Appendix A Broader Impact Our work focuses on exploring whether we can improve the faithfulness of the CoT reasoning generated by state-of-the-art LLMs. This has significant positive implications for societal benefit. For instance, if CoT reasoning output by LLMs faithfully captures the underlying model behavior, decision-makers and relevant stakeholders can leverage this to determine if, when, and how much to rely on the recommendations provided by LLMs. Therefore, our exploration itself is very valuable and has a substantial positive societal impact. Our analyses and findings indicate that existing techniques commonly used to steer behavior in LLMs are not effective in enhancing the faithf

### 2406.10625_on_the_hardness_of_faithful_chain_of_thought_reasoning_in_la_chunk_006.pdf
Table 5: Llama-3-8B-Instruct Faithfulness for Different In-Context Learning Approaches Approach AQuA LogiQA TruthfulQA Accuracy Faithfulness Accuracy Faithfulness Accuracy Faithfulness ZS-CoT 0.43 ± 0.05 0.61 ± 0.02 0.38 ± 0.05 0.41 ± 0.03 0.56 ± 0.05 0.23 ± 0.03 GTA 0.48 ± 0.05 0.64 ± 0.02 0.47 ± 0.05 0.40 ± 0.03 0.57 ± 0.05 0.26 ± 0.03 DU 0.44 ± 0.05 0.65 ± 0.02 0.43 ± 0.05 0.38 ± 0.03 0.59 ± 0.05 0.24 ± 0.03 DUc 0.55 ± 0.05 0.63 ± 0.02 0.47 ± 0.05 0.43 ± 0.03 0.65 ± 0.05 0.23 ± 0.03 DF 0.43 ± 0.05 0.63 ± 0.02 0.35 ± 0.05 0.43 ± 0.03 0.59 ± 0.05 0.24 ± 0.03 DFc 0.37 ± 0.05 0.64 ± 0.02 0.42 ± 0.05 0.39 ± 0.03 0.64 ± 0.05 0.21 ± 0.03 SU 0.52 ± 0.05 0.62 ± 0.02 0.45 ± 0.05 0.42 ± 0.03 0.60 ± 

## 2402.14679_is_self_knowledge_and_action_consistent_or_not_investigating

Total chunks: 4

### 2402.14679_is_self_knowledge_and_action_consistent_or_not_investigating_chunk_001.pdf
Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model’s Personality Yiming Ai 1 Zhiwei He 1 Ziyin Zhang 1 Wenhong Zhu 1 Hongkun Hao 1 Kai Yu2 Lingjun Chen 3 Rui Wang 1 Abstract In this study, we delve into the validity of conven- tional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tenden- cies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we aim to un- derstand the disjunction between self-knowledge and action in

### 2402.14679_is_self_knowledge_and_action_consistent_or_not_investigating_chunk_002.pdf
Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model’s Personality Table 1. LLMs’ Self-knowledge - Action Congruence Performance with Reference of Human Respondents’ Performance (A VG, SD, MIN and MAX represents the average number, standard deviation, minimum and maximum.) LLMs & Human Respondents Cosine Similarity Spearman Rank Correlation Coefficient Value Mean Difference Proportion of Consistent Pairs ChatGLM3 0.24 0.23 1.58 47.22% GPT-3.5-turbo 0.17 0.19 1.74 50.56% GPT-4 0.52 0.56 1.02 78.89% Vicuna-13b 0.08 0.07 1.57 52.78% Vicuna-33b 0.18 0.06 1.68 52.22% LLMs(A VG± SD) 0.24 ± 0.15 0.22 ± 0.18 1.52 ± 0.26 56.78 ± 11.25% Human(A VG± SD) 0.76 ± 0.09 0.78 ± 

### 2402.14679_is_self_knowledge_and_action_consistent_or_not_investigating_chunk_003.pdf
Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model’s Personality A. LLMs’ Unreasonable Responses The unreasonable responses mainly fall into the following five categories: • All responses are the same number; • All responses are greater than or equal to 4 or less than or equal to 4. (Due to the presence of both positive and negative descriptions for the same assessment dimension (e.g., Openness) in our personality knowledge questionnaire, it is impossible for a participant to answer with all responses greater than or equal to 4, indicating agreement or neutrality for all statements, or all responses less than or equal to 4, indicating disagreement or neutralit

### 2402.14679_is_self_knowledge_and_action_consistent_or_not_investigating_chunk_004.pdf
Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model’s Personality Table 5. Several Examples of the Corpus Personality Knowledge Statements Practical Scenario Cases EN: You waste your time. In everyday life: A. you always use your time productively. B. you always spend time on meaningless activities. ZH: 你浪费自己的时间。 在日常生活中： A. 你总是有效地利用时间。 B. 你总是在无意义的活动上花费时间。 EN: You complete tasks successfully. When assigned a challenging project with a tight deadline: A. you are overwhelmed and have difficulty moving the process forward effec- tively, often resulting in incomplete or unsatisfactory results. B. you organise your work and manage your resources properly, and the pro
