# Downloaded Papers

This directory contains papers selected for studying whether Chain-of-Thought (CoT) increases convergence in model reasoning trajectories, internal representations, and persona behavior.

1. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](2201.11903_chain_of_thought_prompting_elicits_reasoning_in_large_langua.pdf)
- Authors: Jason Wei et al.
- Year: 2022
- arXiv: 2201.11903
- Why relevant: Foundational CoT method; establishes baseline CoT behavior and scaling effects.

2. [Large Language Models are Zero-Shot Reasoners](2205.11916_large_language_models_are_zero_shot_reasoners.pdf)
- Authors: Takeshi Kojima et al.
- Year: 2022
- arXiv: 2205.11916
- Why relevant: Introduces zero-shot CoT; useful for prompt-only intervention comparisons.

3. [Self-Consistency Improves Chain of Thought Reasoning in Language Models](2203.11171_self_consistency_improves_chain_of_thought_reasoning_in_lang.pdf)
- Authors: Xuezhi Wang et al.
- Year: 2022
- arXiv: 2203.11171
- Why relevant: Tests whether diverse reasoning paths converge to stable answers.

4. [Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](2305.04388_language_models_don_t_always_say_what_they_think_unfaithful_.pdf)
- Authors: Miles Turpin et al.
- Year: 2023
- arXiv: 2305.04388
- Why relevant: Direct evidence that explicit CoT can diverge from latent decision factors.

5. [Faithful Chain-of-Thought Reasoning](2301.13379_faithful_chain_of_thought_reasoning.pdf)
- Authors: Yaozong Xiao et al.
- Year: 2023
- arXiv: 2301.13379
- Why relevant: Methods to improve alignment between generated rationales and true computation.

6. [On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models](2406.10625_on_the_hardness_of_faithful_chain_of_thought_reasoning_in_la.pdf)
- Authors: Dan Ley et al.
- Year: 2024
- arXiv: 2406.10625
- Why relevant: Explicitly studies faithfulness-accuracy tradeoffs and convergence of partial CoT to final answers.

7. [Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance](2305.17306_chain_of_thought_hub_a_continuous_effort_to_measure_large_la.pdf)
- Authors: Yao Fu et al.
- Year: 2023
- arXiv: 2305.17306
- Why relevant: Benchmark and evaluation infrastructure for CoT-heavy reasoning tasks.

8. [Evaluating Large Language Model Biases in Persona-Steered Generation](2405.20253_evaluating_large_language_model_biases_in_persona_steered_ge.pdf)
- Authors: Malihe Alikhani et al.
- Year: 2024
- arXiv: 2405.20253
- Why relevant: Persona steering behavior and consistency under controlled prompts.

9. [Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model's Personality](2402.14679_is_self_knowledge_and_action_consistent_or_not_investigating.pdf)
- Authors: Yiming Ai et al.
- Year: 2024
- arXiv: 2402.14679
- Why relevant: Quantifies consistency between stated traits and generated behavior.

10. [Self-Cognition in Large Language Models: An Exploratory Study](2407.01505_self_cognition_in_large_language_models_an_exploratory_study.pdf)
- Authors: Weichen Xu et al.
- Year: 2024
- arXiv: 2407.01505
- Why relevant: Measures self-referential stability and internal consistency phenomena.

## Notes

- Metadata for all downloaded papers is in `papers/papers_metadata.json`.
- Paper-finder raw output is in `papers/paper_finder_results.json`.
- Chunked PDFs and deep-reading notes are in `papers/pages/` and `papers/deep_read_chunks_notes.md`.
