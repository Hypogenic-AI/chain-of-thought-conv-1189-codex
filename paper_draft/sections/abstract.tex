\begin{abstract}
Chain-of-thought prompting is widely used to raise reasoning accuracy, but it remains unclear whether it also increases convergence in broader model behavior. We ask a focused question: does chain-of-thought make different frontier models converge more in answers, semantic outputs, and persona expression? We run a controlled API-based study with \gptmodel and \claudemodel on matched subsets of \gsm, \bbhdate, \bbhlogic, and \personachat, comparing \nocot against \cotprompt under fixed seeds and standardized parsing. We evaluate exact-match accuracy, cross-model answer agreement, cross-model embedding cosine similarity, persona adherence, and within-model persona stability under repeated stochastic generations. Results are mixed rather than uniformly positive. CoT improves \gptmodel performance on \gsm (0.600 to 0.900) and raises cross-model agreement on \gsm (0.633 to 0.900), but it does not improve convergence on both BBH subsets. The strongest statistically reliable effect is negative: persona stability for \claudemodel drops from 0.857 to 0.753 ($p=0.00085$, FDR-adjusted $p=0.0145$, Cohen's $d=-0.679$). Most other effects are not significant after FDR correction. These findings support a conditional view of CoT: it can improve convergence on some structured reasoning tasks while degrading persona-level consistency, so deployment decisions should optimize for the specific stability axis that matters.
\end{abstract}
